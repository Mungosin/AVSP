{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cPickle\n",
    "sys.path.append('./models/slim')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from sklearn.decomposition import PCA\n",
    "import tqdm\n",
    "\n",
    "from preprocessing import inception_preprocessing \n",
    "from nets.inception_v4 import inception_v4_arg_scope, inception_v4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded paths\n",
    "* provide full paths to the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint_path = '/home/user/FER/AVSP/inception_v4.ckpt'\n",
    "root_image_folder = '/home/user/Open_image/images_2016_08/train/images'\n",
    "raw_result_folder = '/home/user/FER/AVSP/raw_results'\n",
    "temp_vector_result_folder = '/home/user/FER/AVSP/temp_results'\n",
    "final_result_folder = '/home/user/FER/AVSP/final_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating folders if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(raw_result_folder):\n",
    "    os.makedirs(raw_result_folder)\n",
    "    \n",
    "if not os.path.exists(temp_vector_result_folder):\n",
    "    os.makedirs(temp_vector_result_folder)\n",
    "    \n",
    "if not os.path.exists(final_result_folder):\n",
    "    os.makedirs(final_result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    img = io.imread(path)\n",
    "    img = resize(img,(229,229))\n",
    "    return img\n",
    "\n",
    "def show_img(path):\n",
    "    plt.figure(figsize=(16,12))\n",
    "    img = get_img(path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def maxpool2d(x, k=2, stride=2, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1],\n",
    "                          padding=padding)\n",
    "\n",
    "# This creates a concatenated vector with the result of max pooling all the endpoints with maximum size kernel for them\n",
    "# for example an output of convolution is 12x12x1024, then max pool will be with kernel [1,12,12,1] and padding VALID\n",
    "# the resulting vector will be 1,1,1024 for each image\n",
    "def create_prediction(endpoints):\n",
    "    values_to_concat = []\n",
    "    for end in sorted(endpoints):\n",
    "        if 'Pred' in end or 'Log' in end: \n",
    "            continue\n",
    "        all_channels_maxpool = maxpool2d(end_points[end],end_points[end].shape[1], padding = 'VALID')\n",
    "        values_to_concat.append(all_channels_maxpool)\n",
    "    concatenated = tf.concat(values_to_concat,3)\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "images = tf.placeholder(tf.float32, [None, 229,229,3])\n",
    "\n",
    "with slim.arg_scope(inception_v4_arg_scope()):\n",
    "    logits, end_points = inception_v4(images, num_classes = 1001, is_training = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding unnecessary variables\n",
    "These layers must be excluded from the restored variables otherwise an error is thrown because they're not present in the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = ['InceptionV4/Logits', 'InceptionV4/AuxLogits']\n",
    "exclude = ['InceptionV4/AuxLogits']\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various endpoints contained in the model that will be used for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AuxLogits': <tf.Tensor 'InceptionV4/AuxLogits/Aux_logits/BiasAdd:0' shape=(?, 1001) dtype=float32>,\n",
       " 'Conv2d_1a_3x3': <tf.Tensor 'InceptionV4/InceptionV4/Conv2d_1a_3x3/Relu:0' shape=(?, 114, 114, 32) dtype=float32>,\n",
       " 'Conv2d_2a_3x3': <tf.Tensor 'InceptionV4/InceptionV4/Conv2d_2a_3x3/Relu:0' shape=(?, 112, 112, 32) dtype=float32>,\n",
       " 'Conv2d_2b_3x3': <tf.Tensor 'InceptionV4/InceptionV4/Conv2d_2b_3x3/Relu:0' shape=(?, 112, 112, 64) dtype=float32>,\n",
       " 'Logits': <tf.Tensor 'InceptionV4/Logits/Logits/BiasAdd:0' shape=(?, 1001) dtype=float32>,\n",
       " 'Mixed_3a': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_3a/concat:0' shape=(?, 55, 55, 160) dtype=float32>,\n",
       " 'Mixed_4a': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_4a/concat:0' shape=(?, 53, 53, 192) dtype=float32>,\n",
       " 'Mixed_5a': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_5a/concat:0' shape=(?, 26, 26, 384) dtype=float32>,\n",
       " 'Mixed_5b': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_5b/concat:0' shape=(?, 26, 26, 384) dtype=float32>,\n",
       " 'Mixed_5c': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_5c/concat:0' shape=(?, 26, 26, 384) dtype=float32>,\n",
       " 'Mixed_5d': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_5d/concat:0' shape=(?, 26, 26, 384) dtype=float32>,\n",
       " 'Mixed_5e': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_5e/concat:0' shape=(?, 26, 26, 384) dtype=float32>,\n",
       " 'Mixed_6a': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6a/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6b': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6b/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6c': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6c/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6d': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6d/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6e': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6e/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6f': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6f/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6g': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6g/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_6h': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_6h/concat:0' shape=(?, 12, 12, 1024) dtype=float32>,\n",
       " 'Mixed_7a': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_7a/concat:0' shape=(?, 5, 5, 1536) dtype=float32>,\n",
       " 'Mixed_7b': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_7b/concat:0' shape=(?, 5, 5, 1536) dtype=float32>,\n",
       " 'Mixed_7c': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_7c/concat:0' shape=(?, 5, 5, 1536) dtype=float32>,\n",
       " 'Mixed_7d': <tf.Tensor 'InceptionV4/InceptionV4/Mixed_7d/concat:0' shape=(?, 5, 5, 1536) dtype=float32>,\n",
       " 'PreLogitsFlatten': <tf.Tensor 'InceptionV4/Logits/PreLogitsFlatten/Reshape:0' shape=(?, 1536) dtype=float32>,\n",
       " 'Predictions': <tf.Tensor 'InceptionV4/Logits/Predictions:0' shape=(?, 1001) dtype=float32>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(variables_to_restore)\n",
    "saver.restore(sess, model_checkpoint_path)\n",
    "\n",
    "# Creating the prediction endpoint by maxpooling all the layers\n",
    "pred = create_prediction(end_points)\n",
    "\n",
    "#inputs for preprocessing\n",
    "placeholder_input = tf.placeholder(tf.float32, [None,None,None])\n",
    "preprocessed_img = inception_preprocessing.preprocess_image(placeholder_input, 229, 229, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all paths to images\n",
    "* only reason to do this beforehand is to have a better progress bar with tqdm later, other option would be to do the processing without caching the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths_iterate = []\n",
    "\n",
    "for root, dirs, files in os.walk(root_image_folder):\n",
    "    for name in files:\n",
    "        file_name = os.path.join(root, name)\n",
    "        paths_iterate.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing images and saving partial outputs\n",
    "* change save_every to more/less depending on your RAM\n",
    "* grayscale images are skipped, they could be used if u preprocess them and stack them three times to form a rgb image where all channels have the same value\n",
    "* progress bar with tqdm provides better insight than printing every x iterations\n",
    "* later the chunks will be combined for easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3173a1b2952463a9d2b059a3e17305c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918157/|/ 99%|| 918157/931295 [9:57:06<08:32, 25.63it/s]s]\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "vectors = []\n",
    "cnt = 1\n",
    "part_num = 1\n",
    "save_every = 100000\n",
    "\n",
    "for file_name in tqdm.tqdm_notebook(paths_iterate):\n",
    "    input_img = io.imread(file_name)\n",
    "    input_img = resize(input_img, (229,229), mode = 'constant')\n",
    "    if len(input_img.shape)!=3:\n",
    "        #Skipping grayscale images\n",
    "        continue\n",
    "\n",
    "    test = sess.run(preprocessed_img, feed_dict={placeholder_input:input_img})\n",
    "    res = sess.run(pred,feed_dict={images:[test]})[0].flatten()\n",
    "    paths.append(file_name)\n",
    "    vectors.append(res)\n",
    "\n",
    "    if cnt % save_every == 0:\n",
    "        vectors = np.array(vectors)\n",
    "        raw_result_folder\n",
    "        vector_location = os.path.join(raw_result_folder,'vectors_%03d' % part_num)\n",
    "        np.save(vector_location, vectors)\n",
    "        \n",
    "        paths = np.array(paths)\n",
    "        path_location = os.path.join(raw_result_folder,'paths_%03d' % part_num)\n",
    "        np.save(path_location, paths)\n",
    "        part_num += 1\n",
    "        vectors = []\n",
    "        paths = []\n",
    "\n",
    "    cnt+=1\n",
    "    \n",
    "#saving last results\n",
    "vectors = np.array(vectors)\n",
    "vector_location = os.path.join(raw_result_folder,'vectors_%03d' % part_num)\n",
    "np.save(vector_location, vectors)\n",
    "\n",
    "paths = np.array(paths)\n",
    "path_location = os.path.join(raw_result_folder,'paths_%03d' % part_num)\n",
    "np.save(path_location, paths)\n",
    "\n",
    "paths_iterate = []\n",
    "vectors = []\n",
    "paths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting PCA model\n",
    "* uncomment saving/fitting part depending if u trained it before or you already have the model and you're restoring it from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Partial vectors for PCA model training, memory requirements are around 13Gb for these so you can\n",
    "#combine more parts if you have more memory and train on those\n",
    "vector_location = os.path.join(raw_result_folder,'vectors_001.npy')\n",
    "vectors = np.load(vector_location)\n",
    "\n",
    "\n",
    "PCA_location = os.path.join(final_result_folder,'PCA_model_300.pkl')\n",
    "PCA_model = PCA(n_components=300)\n",
    "\n",
    "#careful to comment the fit and saving parts back after training so you dont overwrite your results \n",
    "#if you're running whole notebook at once\n",
    "PCA_model.fit(vectors)\n",
    "\n",
    "with open(PCA_location, 'wb') as f:\n",
    "    cPickle.dump(PCA_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing all the raw results with PCA and storing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/FER/AVSP/raw_results/vectors_001.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_002.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_003.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_004.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_005.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_006.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_007.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_008.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_009.npy\n",
      "/home/user/FER/AVSP/raw_results/vectors_010.npy\n"
     ]
    }
   ],
   "source": [
    "PCA_model = PCA(n_components=300)\n",
    "PCA_location = os.path.join(final_result_folder,'PCA_model_300.pkl')\n",
    "\n",
    "with open(PCA_location, 'rb') as f:\n",
    "    PCA_model = cPickle.load(f)\n",
    "    \n",
    "for root, dirs, files in os.walk(raw_result_folder):\n",
    "    for name in sorted(files):\n",
    "        if 'vector' not in name:\n",
    "            continue\n",
    "        file_name = os.path.join(root, name)\n",
    "        print file_name\n",
    "        vectors = np.load(file_name)\n",
    "        vectors = PCA_model.transform(vectors)\n",
    "        np.save(os.path.join(temp_vector_result_folder, name), vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the reduced results and storing it as a single numpy matrix for easier usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining paths to single npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/FER/AVSP/raw_results/paths_001.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_002.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_003.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_004.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_005.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_006.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_007.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_008.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_009.npy\n",
      "(100000,)\n",
      "/home/user/FER/AVSP/raw_results/paths_010.npy\n",
      "(12891,)\n",
      "(912891,)\n"
     ]
    }
   ],
   "source": [
    "paths = None\n",
    "for root, dirs, files in os.walk(raw_result_folder):\n",
    "    for name in sorted(files):\n",
    "        if 'path' not in name:\n",
    "            continue\n",
    "        file_name = os.path.join(root, name)\n",
    "        print file_name\n",
    "        temp = np.load(file_name)\n",
    "        if isinstance(paths, np.ndarray):\n",
    "            paths = np.concatenate((paths, temp), axis = 0)\n",
    "        else:\n",
    "            paths = temp\n",
    "final_result_path = os.path.join(final_result_folder, 'full_paths')\n",
    "np.save(final_result_path, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining vectors to single npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/FER/AVSP/temp_results/vectors_001.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_002.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_003.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_004.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_005.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_006.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_007.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_008.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_009.npy\n",
      "/home/user/FER/AVSP/temp_results/vectors_010.npy\n"
     ]
    }
   ],
   "source": [
    "vectors = None\n",
    "for root, dirs, files in os.walk(temp_vector_result_folder):\n",
    "    for name in sorted(files):\n",
    "        if 'vector' not in name:\n",
    "            continue\n",
    "        file_name = os.path.join(root, name)\n",
    "        print file_name\n",
    "        temp = np.load(file_name)\n",
    "        if isinstance(vectors, np.ndarray):\n",
    "            vectors = np.concatenate((vectors, temp), axis = 0)\n",
    "        else:\n",
    "            vectors = temp\n",
    "            \n",
    "final_result_path = os.path.join(final_result_folder, 'full_vectors')\n",
    "np.save(final_result_path,vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AVSP",
   "language": "python",
   "name": "avsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
